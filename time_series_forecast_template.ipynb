{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 時系列データ予測テンプレート (Time Series Forecasting Template) - Ultimate Edition\n",
                "\n",
                "このノートブックは、2025年のデータサイエンス業界標準に基づいた時系列予測の決定版テンプレートです。\n",
                "基本的なフローに加え、高度な欠損値補完、不均衡データ対策、**包括的なPandasデータ加工**、**テキストデータ処理**、**多様な機械学習モデル**、そして**SHAPによるモデル解釈**の実装例を含んでいます。\n",
                "\n",
                "## 目次\n",
                "1. [設定とライブラリのインポート](#1.-設定とライブラリのインポート)\n",
                "2. [データの読み込み](#2.-データの読み込み)\n",
                "3. [データの前処理とクレンジング (Advanced)](#3.-データの前処理とクレンジング-(Advanced))\n",
                "4. [高度なデータ加工 (Comprehensive Pandas & Text)](#4.-高度なデータ加工-(Comprehensive-Pandas-&-Text))\n",
                "5. [探索的データ分析 (EDA)](#5.-探索的データ分析-(EDA))\n",
                "6. [特徴量エンジニアリング](#6.-特徴量エンジニアリング)\n",
                "7. [特徴量選択](#7.-特徴量選択)\n",
                "8. [モデル学習と検証 (回帰 - Multi-Model)](#8.-モデル学習と検証-(回帰---Multi-Model))\n",
                "9. [モデル解釈 (SHAP Analysis)](#9.-モデル解釈-(SHAP-Analysis))\n",
                "10. [モデル学習と検証 (分類 - Imbalanced Data)](#10.-モデル学習と検証-(分類---Imbalanced-Data))\n",
                "11. [最終評価と結果の保存](#11.-最終評価と結果の保存)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. 設定とライブラリのインポート\n",
                "必要なライブラリをインポートし、表示設定や乱数シードの固定を行います。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import glob\n",
                "import warnings\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Scikit-learn modules\n",
                "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, RandomizedSearchCV\n",
                "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.impute import SimpleImputer, KNNImputer\n",
                "from sklearn.experimental import enable_iterative_imputer\n",
                "from sklearn.impute import IterativeImputer\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.feature_selection import RFE, SelectFromModel\n",
                "from sklearn.inspection import permutation_importance\n",
                "# Models\n",
                "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, VotingRegressor, HistGradientBoostingRegressor\n",
                "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
                "from sklearn.svm import SVR\n",
                "from sklearn.utils import resample\n",
                "from sklearn.metrics import (\n",
                "    mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error,\n",
                "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
                ")\n",
                "\n",
                "# SHAP (Model Interpretability)\n",
                "try:\n",
                "    import shap\n",
                "except ImportError:\n",
                "    print(\"SHAP library not found. Please install it using 'pip install shap'\")\n",
                "\n",
                "# 設定\n",
                "warnings.filterwarnings('ignore')\n",
                "pd.set_option('display.max_columns', None)\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "\n",
                "# 日本語フォントの設定（環境に合わせて変更してください）\n",
                "# plt.rcParams['font.family'] = 'Meiryo' \n",
                "\n",
                "# 乱数シードの固定\n",
                "SEED = 42\n",
                "np.random.seed(SEED)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. データの読み込み\n",
                "指定されたディレクトリ内の複数のCSVファイルを読み込み、結合します。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_data(data_dir, file_pattern='*.csv'):\n",
                "    \"\"\"\n",
                "    指定ディレクトリから複数のCSVを読み込み、結合して返す関数\n",
                "    \"\"\"\n",
                "    files = glob.glob(os.path.join(data_dir, file_pattern))\n",
                "    if not files:\n",
                "        raise FileNotFoundError(f\"No CSV files found in {data_dir}\")\n",
                "    \n",
                "    df_list = []\n",
                "    for file in files:\n",
                "        print(f\"Loading: {file}\")\n",
                "        temp_df = pd.read_csv(file)\n",
                "        df_list.append(temp_df)\n",
                "    \n",
                "    combined_df = pd.concat(df_list, axis=0, ignore_index=True)\n",
                "    return combined_df\n",
                "\n",
                "# ダミーデータ生成（不均衡データ・テキストデータを含む）\n",
                "def generate_dummy_data():\n",
                "    dates = pd.date_range(start='2023-01-01', end='2024-12-31', freq='D')\n",
                "    n = len(dates)\n",
                "    \n",
                "    # テキストデータのダミー生成\n",
                "    log_messages = [\n",
                "        \"INFO: System Normal\",\n",
                "        \"WARNING: High Latency (120ms)\",\n",
                "        \"ERROR: Connection Failed (Code: 503)\",\n",
                "        \"INFO: Maintenance Started\",\n",
                "        \"INFO: Maintenance Completed\"\n",
                "    ]\n",
                "    \n",
                "    df = pd.DataFrame({\n",
                "        'date': dates,\n",
                "        'target': np.sin(np.linspace(0, 20, n)) + np.random.normal(0, 0.1, n) + np.linspace(0, 5, n),\n",
                "        'feature_1': np.random.rand(n) * 100,\n",
                "        'feature_2': np.random.randint(0, 10, n),\n",
                "        'feature_missing': np.where(np.random.rand(n) > 0.8, np.nan, np.random.rand(n)),\n",
                "        'category_col': np.random.choice(['A', 'B', 'C'], n),\n",
                "        'log_message': np.random.choice(log_messages, n) # テキストカラム\n",
                "    })\n",
                "    return df\n",
                "\n",
                "df = generate_dummy_data()\n",
                "print(f\"Data Shape: {df.shape}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. データの前処理とクレンジング (Advanced)\n",
                "基本的な欠損処理に加え、KNNImputerやIterativeImputerを用いた高度な補完を行います。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 日付カラムの変換とインデックス設定\n",
                "date_col = 'date'\n",
                "if date_col in df.columns:\n",
                "    df[date_col] = pd.to_datetime(df[date_col])\n",
                "    df = df.sort_values(date_col).set_index(date_col)\n",
                "\n",
                "# 欠損値の確認\n",
                "print(\"Missing Values Before Imputation:\\n\", df.isnull().sum())\n",
                "\n",
                "# --- Advanced Imputation Strategies ---\n",
                "# 数値列の抽出\n",
                "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
                "\n",
                "# 戦略1: KNN Imputer (近傍探索による補完)\n",
                "imputer_knn = KNNImputer(n_neighbors=5)\n",
                "df_knn_imputed = pd.DataFrame(imputer_knn.fit_transform(df[numeric_cols]), columns=numeric_cols, index=df.index)\n",
                "\n",
                "# 戦略2: Iterative Imputer (多変量回帰による連鎖的な補完)\n",
                "imputer_iter = IterativeImputer(max_iter=10, random_state=SEED)\n",
                "df_iter_imputed = pd.DataFrame(imputer_iter.fit_transform(df[numeric_cols]), columns=numeric_cols, index=df.index)\n",
                "\n",
                "# ここでは Iterative Imputer の結果を採用します\n",
                "df[numeric_cols] = df_iter_imputed\n",
                "\n",
                "# カテゴリ変数のエンコーディング\n",
                "df = pd.get_dummies(df, columns=['category_col'], drop_first=True)\n",
                "\n",
                "print(\"Missing Values After Imputation:\\n\", df.isnull().sum())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. 高度なデータ加工 (Comprehensive Pandas & Text)\n",
                "Pandasの強力な機能を用いたデータ変換、リサンプリング、集約処理、および**テキストデータ処理**の例です。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Text Data Processing (New) ---\n",
                "\n",
                "# 1. 正規表現 (Regex) による抽出\n",
                "# 'log_message' からエラーコードなどを抽出する例\n",
                "# パターン: 'Code: ' の後ろの数字を抽出\n",
                "df['error_code'] = df['log_message'].str.extract(r'Code: (\\d+)')\n",
                "df['error_code'] = df['error_code'].fillna(0).astype(int) # 欠損は0として扱う\n",
                "\n",
                "# 2. 文字列操作とフラグ作成\n",
                "# 特定のキーワードが含まれているかを判定\n",
                "df['is_error'] = df['log_message'].str.contains('ERROR', case=False).astype(int)\n",
                "df['is_maintenance'] = df['log_message'].str.contains('Maintenance', case=False).astype(int)\n",
                "\n",
                "# 3. テキストのクリーニング\n",
                "# 小文字化、空白除去など\n",
                "df['clean_message'] = df['log_message'].str.lower().str.strip()\n",
                "\n",
                "# 4. TF-IDF Vectorization (テキストの数値化)\n",
                "# 重要な単語を特徴量として抽出\n",
                "tfidf = TfidfVectorizer(max_features=5, stop_words='english')\n",
                "tfidf_matrix = tfidf.fit_transform(df['clean_message'])\n",
                "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=[f'tfidf_{i}' for i in range(tfidf_matrix.shape[1])], index=df.index)\n",
                "\n",
                "# 元のデータフレームに結合\n",
                "df = pd.concat([df, tfidf_df], axis=1)\n",
                "\n",
                "# 不要になったテキストカラムを削除\n",
                "df.drop(columns=['log_message', 'clean_message'], inplace=True)\n",
                "\n",
                "# --- Pandas Advanced Processing ---\n",
                "\n",
                "# 5. Resampling\n",
                "df_weekly = df.resample('W').agg({\n",
                "    'target': ['mean', 'sum', 'max'],\n",
                "    'feature_1': 'mean'\n",
                "})\n",
                "\n",
                "# 6. Rolling Window\n",
                "df['rolling_mean_7d'] = df['target'].rolling('7D').mean()\n",
                "df['rolling_std_7d'] = df['target'].rolling('7D').std()\n",
                "\n",
                "# 7. EWMA\n",
                "df['ewm_mean_span7'] = df['target'].ewm(span=7).mean()\n",
                "\n",
                "# 8. Groupby + Transform\n",
                "df['month'] = df.index.month\n",
                "df['target_month_demeaned'] = df.groupby('month')['target'].transform(lambda x: x - x.mean())\n",
                "\n",
                "# 9. Differencing & Percent Change\n",
                "df['target_diff'] = df['target'].diff()\n",
                "df['target_pct_change'] = df['target'].pct_change()\n",
                "\n",
                "# 10. Expanding Window\n",
                "df['expanding_max'] = df['target'].expanding().max()\n",
                "df['expanding_mean'] = df['target'].expanding().mean()\n",
                "\n",
                "# 11. Binning\n",
                "df['target_bin'] = pd.qcut(df['target'], q=3, labels=['Low', 'Medium', 'High'])\n",
                "\n",
                "# 12. Merge AsOf (Example)\n",
                "economic_data = pd.DataFrame({\n",
                "    'date': pd.date_range(start='2023-01-01', end='2024-12-31', freq='MS'),\n",
                "    'gdp_growth': np.random.rand(24)\n",
                "})\n",
                "df_sorted = df.sort_index()\n",
                "economic_data_sorted = economic_data.sort_values('date')\n",
                "df_merged = pd.merge_asof(df_sorted, economic_data_sorted, \n",
                "                          left_index=True, right_on='date', \n",
                "                          direction='backward')\n",
                "df_merged = df_merged.set_index(df_sorted.index)\n",
                "df = df_merged.drop(columns=['date', 'target_bin'])\n",
                "\n",
                "df.drop(columns=['month'], inplace=True)\n",
                "df.dropna(inplace=True)\n",
                "\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. 探索的データ分析 (EDA)\n",
                "データの傾向、季節性、相関関係を可視化します。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "target_col = 'target'\n",
                "\n",
                "plt.figure(figsize=(15, 5))\n",
                "plt.plot(df.index, df[target_col], label='Target')\n",
                "plt.plot(df.index, df['ewm_mean_span7'], label='EWMA (Span 7)', linestyle='--', alpha=0.8)\n",
                "plt.plot(df.index, df['expanding_mean'], label='Expanding Mean', linestyle=':', alpha=0.8)\n",
                "plt.title('Time Series Plot with EWMA and Expanding Mean')\n",
                "plt.legend()\n",
                "plt.show()\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(df.corr(), annot=False, cmap='coolwarm')\n",
                "plt.title('Correlation Matrix')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. 特徴量エンジニアリング\n",
                "時系列特有の特徴量を作成します。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_features(data, target_col):\n",
                "    df_feat = data.copy()\n",
                "    \n",
                "    df_feat['month'] = df_feat.index.month\n",
                "    df_feat['day'] = df_feat.index.day\n",
                "    df_feat['dayofweek'] = df_feat.index.dayofweek\n",
                "    \n",
                "    lags = [1, 7, 30]\n",
                "    for lag in lags:\n",
                "        df_feat[f'lag_{lag}'] = df_feat[target_col].shift(lag)\n",
                "        \n",
                "    df_feat = df_feat.dropna()\n",
                "    return df_feat\n",
                "\n",
                "df_processed = create_features(df, target_col)\n",
                "print(f\"Processed Data Shape: {df_processed.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. 特徴量選択\n",
                "Permutation Importanceを用いたより信頼性の高い特徴量重要度の算出を行います。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = df_processed.drop(columns=[target_col])\n",
                "y = df_processed[target_col]\n",
                "\n",
                "# Permutation Importance\n",
                "rf = RandomForestRegressor(n_estimators=50, random_state=SEED, n_jobs=-1)\n",
                "rf.fit(X, y)\n",
                "\n",
                "result = permutation_importance(rf, X, y, n_repeats=10, random_state=SEED, n_jobs=-1)\n",
                "perm_sorted_idx = result.importances_mean.argsort()[::-1]\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "sns.boxplot(data=result.importances[perm_sorted_idx].T, orient='h')\n",
                "plt.yticks(range(len(X.columns)), X.columns[perm_sorted_idx])\n",
                "plt.title(\"Permutation Importance\")\n",
                "plt.show()\n",
                "\n",
                "# 上位特徴量の選択\n",
                "top_features = X.columns[perm_sorted_idx][:10].tolist()\n",
                "X_selected = X[top_features]\n",
                "print(f\"Selected Features: {top_features}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. モデル学習と検証 (回帰 - Multi-Model)\n",
                "多様なモデル（HistGradientBoosting, SVR, Linear Models）を比較・検証します。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tscv = TimeSeriesSplit(n_splits=5)\n",
                "\n",
                "# 比較するモデルの定義\n",
                "models = {\n",
                "    'Linear Regression': LinearRegression(),\n",
                "    'Ridge': Ridge(alpha=1.0),\n",
                "    'Lasso': Lasso(alpha=0.1),\n",
                "    'SVR': SVR(kernel='rbf', C=1.0, epsilon=0.1),\n",
                "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=SEED),\n",
                "    'Hist Gradient Boosting': HistGradientBoostingRegressor(random_state=SEED) # 高速で欠損値も扱える\n",
                "}\n",
                "\n",
                "results = {}\n",
                "\n",
                "print(\"--- Model Comparison (CV RMSE) ---\")\n",
                "for name, model in models.items():\n",
                "    # SVRなどはスケーリングが必須のためパイプライン化\n",
                "    pipeline = Pipeline([\n",
                "        ('scaler', StandardScaler()),\n",
                "        ('regressor', model)\n",
                "    ])\n",
                "    \n",
                "    cv_scores = cross_val_score(pipeline, X_selected, y, cv=tscv, scoring='neg_mean_squared_error')\n",
                "    rmse_scores = np.sqrt(-cv_scores)\n",
                "    mean_rmse = rmse_scores.mean()\n",
                "    results[name] = mean_rmse\n",
                "    print(f\"{name}: {mean_rmse:.4f}\")\n",
                "\n",
                "# ベストモデルの選択\n",
                "best_model_name = min(results, key=results.get)\n",
                "print(f\"\\nBest Model: {best_model_name}\")\n",
                "\n",
                "# ベストモデルでの最終学習\n",
                "best_model = models[best_model_name]\n",
                "final_pipeline = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('regressor', best_model)\n",
                "])\n",
                "\n",
                "split_idx = int(len(X_selected) * 0.8)\n",
                "X_train, X_test = X_selected.iloc[:split_idx], X_selected.iloc[split_idx:]\n",
                "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
                "\n",
                "final_pipeline.fit(X_train, y_train)\n",
                "y_pred = final_pipeline.predict(X_test)\n",
                "\n",
                "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
                "print(f\"Final Test RMSE ({best_model_name}): {rmse:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. モデル解釈 (SHAP Analysis)\n",
                "SHAP (SHapley Additive exPlanations) を用いて、モデルの予測根拠を可視化します。\n",
                "※ SHAPライブラリが必要です (`pip install shap`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    import shap\n",
                "    \n",
                "    # SHAPはTree系モデルで最も効果的かつ高速です\n",
                "    # 説明用にRandomForestモデルを再学習（または既存のTreeモデルを使用）\n",
                "    explainer_model = RandomForestRegressor(n_estimators=100, random_state=SEED)\n",
                "    explainer_model.fit(X_train, y_train)\n",
                "    \n",
                "    # Explainerの作成\n",
                "    explainer = shap.TreeExplainer(explainer_model)\n",
                "    shap_values = explainer.shap_values(X_test)\n",
                "    \n",
                "    print(\"--- SHAP Summary Plot ---\")\n",
                "    # 特徴量重要度と影響の方向性を表示\n",
                "    plt.figure(figsize=(10, 6))\n",
                "    shap.summary_plot(shap_values, X_test, show=False)\n",
                "    plt.title('SHAP Summary Plot')\n",
                "    plt.show()\n",
                "    \n",
                "    print(\"--- SHAP Dependence Plot ---\")\n",
                "    # 最も重要な特徴量とターゲットの関係（交互作用含む）を表示\n",
                "    top_feature = X_test.columns[np.abs(shap_values).mean(0).argmax()]\n",
                "    shap.dependence_plot(top_feature, shap_values, X_test, show=False)\n",
                "    plt.title(f'SHAP Dependence Plot: {top_feature}')\n",
                "    plt.show()\n",
                "    \n",
                "    print(\"--- SHAP Waterfall Plot (Local Explanation) ---\")\n",
                "    # 特定の予測（例：最初のテストデータ）に対する各特徴量の寄与を表示\n",
                "    # shap.plots.waterfall は explainer(X) の結果オブジェクトを必要とします\n",
                "    # バージョン互換性のため、ここでは汎用的な force_plot を使用する場合もありますが、\n",
                "    # 最新のSHAPでは waterfall が推奨されます。\n",
                "    \n",
                "    # Explanationオブジェクトの作成\n",
                "    explanation = shap.Explanation(values=shap_values, \n",
                "                                   base_values=explainer.expected_value, \n",
                "                                   data=X_test, \n",
                "                                   feature_names=X_test.columns)\n",
                "    \n",
                "    plt.figure(figsize=(10, 6))\n",
                "    shap.plots.waterfall(explanation[0], show=False)\n",
                "    plt.title('SHAP Waterfall Plot (First Sample)')\n",
                "    plt.show()\n",
                "\n",
                "except ImportError:\n",
                "    print(\"SHAP is not installed. Skipping SHAP analysis.\")\n",
                "except Exception as e:\n",
                "    print(f\"An error occurred during SHAP analysis: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. モデル学習と検証 (分類 - Imbalanced Data)\n",
                "不均衡データ（少数クラス）への対策として、重み付け (Class Weight) とリサンプリングを行います。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 分類ターゲットの作成（例：大きな上昇を予測するタスク、サンプル数が少ないと仮定）\n",
                "threshold = y.quantile(0.9) # 上位10%を「急騰」とする\n",
                "y_class = (y.shift(-1) > threshold).astype(int).iloc[:-1]\n",
                "X_class = X_selected.iloc[:-1]\n",
                "\n",
                "print(\"Class Distribution:\\n\", y_class.value_counts(normalize=True))\n",
                "\n",
                "# 分割\n",
                "split_idx_class = int(len(X_class) * 0.8)\n",
                "X_train_c, X_test_c = X_class.iloc[:split_idx_class], X_class.iloc[split_idx_class:]\n",
                "y_train_c, y_test_c = y_class.iloc[:split_idx_class], y_class.iloc[split_idx_class:]\n",
                "\n",
                "# 対策1: Class Weight Balanced\n",
                "clf_balanced = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=SEED)\n",
                "clf_balanced.fit(X_train_c, y_train_c)\n",
                "y_pred_balanced = clf_balanced.predict(X_test_c)\n",
                "\n",
                "print(\"\\n--- Balanced Class Weight Results ---\")\n",
                "print(classification_report(y_test_c, y_pred_balanced))\n",
                "\n",
                "# 対策2: Upsampling Minority Class (リサンプリング)\n",
                "train_data = pd.concat([X_train_c, y_train_c], axis=1)\n",
                "majority = train_data[train_data[y_train_c.name] == 0]\n",
                "minority = train_data[train_data[y_train_c.name] == 1]\n",
                "\n",
                "minority_upsampled = resample(minority, \n",
                "                              replace=True,     # サンプルを重複させる\n",
                "                              n_samples=len(majority),    # 多数派と同じ数まで増やす\n",
                "                              random_state=SEED)\n",
                "\n",
                "train_upsampled = pd.concat([majority, minority_upsampled])\n",
                "X_train_up = train_upsampled.drop(columns=[y_train_c.name])\n",
                "y_train_up = train_upsampled[y_train_c.name]\n",
                "\n",
                "clf_resampled = RandomForestClassifier(n_estimators=100, random_state=SEED)\n",
                "clf_resampled.fit(X_train_up, y_train_up)\n",
                "y_pred_resampled = clf_resampled.predict(X_test_c)\n",
                "\n",
                "print(\"\\n--- Upsampling Results ---\")\n",
                "print(classification_report(y_test_c, y_pred_resampled))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. 最終評価と結果の保存\n",
                "回帰モデルの予測結果を可視化し、結果をCSVとして保存します。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(15, 6))\n",
                "plt.plot(y_test.index, y_test, label='Actual', alpha=0.7)\n",
                "plt.plot(y_test.index, y_pred, label='Predicted', alpha=0.7, linestyle='--')\n",
                "plt.title('Actual vs Predicted')\n",
                "plt.legend()\n",
                "plt.show()\n",
                "\n",
                "results_df = pd.DataFrame({\n",
                "    'Actual': y_test,\n",
                "    'Predicted': y_pred\n",
                "})\n",
                "results_df.to_csv('forecast_results_ultimate.csv')\n",
                "print(\"Results saved to forecast_results_ultimate.csv\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}