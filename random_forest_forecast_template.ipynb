{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 時系列データ予測テンプレート (Random Forest Edition)\n",
                "\n",
                "このノートブックは、**ランダムフォレスト (Random Forest)** に特化した時系列予測テンプレートです。\n",
                "強力で解釈性の高いランダムフォレストモデルを使用し、高度なデータ前処理、テキスト処理、特徴量エンジニアリング、そしてSHAPによる詳細なモデル解釈までを一貫して行います。\n",
                "\n",
                "## 目次\n",
                "1. [設定とライブラリのインポート](#1.-設定とライブラリのインポート)\n",
                "2. [データの読み込み](#2.-データの読み込み)\n",
                "3. [データの前処理とクレンジング (Advanced)](#3.-データの前処理とクレンジング-(Advanced))\n",
                "4. [高度なデータ加工 (Comprehensive Pandas & Text)](#4.-高度なデータ加工-(Comprehensive-Pandas-&-Text))\n",
                "5. [探索的データ分析 (EDA)](#5.-探索的データ分析-(EDA))\n",
                "6. [特徴量エンジニアリング](#6.-特徴量エンジニアリング)\n",
                "7. [特徴量選択](#7.-特徴量選択)\n",
                "8. [モデル学習と検証 (回帰 - Random Forest)](#8.-モデル学習と検証-(回帰---Random-Forest))\n",
                "9. [モデル解釈 (SHAP Analysis)](#9.-モデル解釈-(SHAP-Analysis))\n",
                "10. [モデル学習と検証 (分類 - Imbalanced Data)](#10.-モデル学習と検証-(分類---Imbalanced-Data))\n",
                "11. [最終評価と結果の保存](#11.-最終評価と結果の保存)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. 設定とライブラリのインポート\n",
                "必要なライブラリをインポートします。ここではランダムフォレストに焦点を当てています。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import glob\n",
                "import warnings\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Scikit-learn modules\n",
                "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, RandomizedSearchCV\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.impute import KNNImputer\n",
                "from sklearn.experimental import enable_iterative_imputer\n",
                "from sklearn.impute import IterativeImputer\n",
                "from sklearn.inspection import permutation_importance\n",
                "\n",
                "# Models (Random Forest Only)\n",
                "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
                "from sklearn.utils import resample\n",
                "from sklearn.metrics import (\n",
                "    mean_squared_error, mean_absolute_error, r2_score,\n",
                "    accuracy_score, classification_report, confusion_matrix\n",
                ")\n",
                "\n",
                "# SHAP (Model Interpretability)\n",
                "try:\n",
                "    import shap\n",
                "except ImportError:\n",
                "    print(\"SHAP library not found. Please install it using 'pip install shap'\")\n",
                "\n",
                "# 設定\n",
                "warnings.filterwarnings('ignore')\n",
                "pd.set_option('display.max_columns', None)\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "\n",
                "# 乱数シードの固定\n",
                "SEED = 42\n",
                "np.random.seed(SEED)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. データの読み込み\n",
                "ダミーデータを生成して使用します。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_dummy_data():\n",
                "    dates = pd.date_range(start='2023-01-01', end='2024-12-31', freq='D')\n",
                "    n = len(dates)\n",
                "    \n",
                "    # テキストデータのダミー生成\n",
                "    log_messages = [\n",
                "        \"INFO: System Normal\",\n",
                "        \"WARNING: High Latency (120ms)\",\n",
                "        \"ERROR: Connection Failed (Code: 503)\",\n",
                "        \"INFO: Maintenance Started\",\n",
                "        \"INFO: Maintenance Completed\"\n",
                "    ]\n",
                "    \n",
                "    df = pd.DataFrame({\n",
                "        'date': dates,\n",
                "        'target': np.sin(np.linspace(0, 20, n)) + np.random.normal(0, 0.1, n) + np.linspace(0, 5, n),\n",
                "        'feature_1': np.random.rand(n) * 100,\n",
                "        'feature_2': np.random.randint(0, 10, n),\n",
                "        'feature_missing': np.where(np.random.rand(n) > 0.8, np.nan, np.random.rand(n)),\n",
                "        'category_col': np.random.choice(['A', 'B', 'C'], n),\n",
                "        'log_message': np.random.choice(log_messages, n)\n",
                "    })\n",
                "    return df\n",
                "\n",
                "df = generate_dummy_data()\n",
                "print(f\"Data Shape: {df.shape}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. データの前処理とクレンジング (Advanced)\n",
                "Iterative Imputerなどを用いた高度な欠損値補完を行います。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "date_col = 'date'\n",
                "if date_col in df.columns:\n",
                "    df[date_col] = pd.to_datetime(df[date_col])\n",
                "    df = df.sort_values(date_col).set_index(date_col)\n",
                "\n",
                "# 数値列の抽出と補完\n",
                "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
                "imputer_iter = IterativeImputer(max_iter=10, random_state=SEED)\n",
                "df[numeric_cols] = imputer_iter.fit_transform(df[numeric_cols])\n",
                "\n",
                "# カテゴリ変数のエンコーディング\n",
                "df = pd.get_dummies(df, columns=['category_col'], drop_first=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. 高度なデータ加工 (Comprehensive Pandas & Text)\n",
                "テキスト処理（Regex, TF-IDF）とPandasによる時系列特徴量の作成を行います。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Text Data Processing ---\n",
                "df['error_code'] = df['log_message'].str.extract(r'Code: (\\d+)').fillna(0).astype(int)\n",
                "df['is_error'] = df['log_message'].str.contains('ERROR', case=False).astype(int)\n",
                "df['clean_message'] = df['log_message'].str.lower().str.strip()\n",
                "\n",
                "tfidf = TfidfVectorizer(max_features=5, stop_words='english')\n",
                "tfidf_df = pd.DataFrame(tfidf.fit_transform(df['clean_message']).toarray(), \n",
                "                        columns=[f'tfidf_{i}' for i in range(5)], index=df.index)\n",
                "df = pd.concat([df, tfidf_df], axis=1)\n",
                "df.drop(columns=['log_message', 'clean_message'], inplace=True)\n",
                "\n",
                "# --- Pandas Advanced Processing ---\n",
                "df['rolling_mean_7d'] = df['target'].rolling('7D').mean()\n",
                "df['ewm_mean_span7'] = df['target'].ewm(span=7).mean()\n",
                "df['target_diff'] = df['target'].diff()\n",
                "df['expanding_max'] = df['target'].expanding().max()\n",
                "\n",
                "df.dropna(inplace=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. 探索的データ分析 (EDA)\n",
                "ターゲット変数の推移を確認します。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(15, 5))\n",
                "plt.plot(df.index, df['target'], label='Target')\n",
                "plt.plot(df.index, df['ewm_mean_span7'], label='EWMA (Span 7)', linestyle='--')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. 特徴量エンジニアリング\n",
                "ラグ特徴量などを作成します。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_features(data, target_col):\n",
                "    df_feat = data.copy()\n",
                "    df_feat['month'] = df_feat.index.month\n",
                "    df_feat['dayofweek'] = df_feat.index.dayofweek\n",
                "    \n",
                "    for lag in [1, 7, 30]:\n",
                "        df_feat[f'lag_{lag}'] = df_feat[target_col].shift(lag)\n",
                "        \n",
                "    return df_feat.dropna()\n",
                "\n",
                "df_processed = create_features(df, 'target')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. 特徴量選択\n",
                "Random Forestを用いたPermutation Importanceで特徴量を選別します。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = df_processed.drop(columns=['target'])\n",
                "y = df_processed['target']\n",
                "\n",
                "rf = RandomForestRegressor(n_estimators=50, random_state=SEED, n_jobs=-1)\n",
                "rf.fit(X, y)\n",
                "\n",
                "result = permutation_importance(rf, X, y, n_repeats=10, random_state=SEED, n_jobs=-1)\n",
                "perm_sorted_idx = result.importances_mean.argsort()[::-1]\n",
                "top_features = X.columns[perm_sorted_idx][:10].tolist()\n",
                "X_selected = X[top_features]\n",
                "\n",
                "print(f\"Selected Features: {top_features}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. モデル学習と検証 (回帰 - Random Forest)\n",
                "RandomForestRegressorを使用し、ハイパーパラメータチューニングを行います。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tscv = TimeSeriesSplit(n_splits=5)\n",
                "\n",
                "# パラメータグリッド\n",
                "param_dist = {\n",
                "    'regressor__n_estimators': [100, 200, 300],\n",
                "    'regressor__max_depth': [None, 10, 20, 30],\n",
                "    'regressor__min_samples_split': [2, 5, 10],\n",
                "    'regressor__min_samples_leaf': [1, 2, 4]\n",
                "}\n",
                "\n",
                "pipeline = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('regressor', RandomForestRegressor(random_state=SEED))\n",
                "])\n",
                "\n",
                "search = RandomizedSearchCV(\n",
                "    pipeline, \n",
                "    param_distributions=param_dist, \n",
                "    n_iter=10, \n",
                "    cv=tscv, \n",
                "    scoring='neg_mean_squared_error', \n",
                "    random_state=SEED, \n",
                "    n_jobs=-1\n",
                ")\n",
                "\n",
                "search.fit(X_selected, y)\n",
                "print(f\"Best Parameters: {search.best_params_}\")\n",
                "print(f\"Best CV RMSE: {np.sqrt(-search.best_score_):.4f}\")\n",
                "\n",
                "# 最終モデルの学習\n",
                "best_model = search.best_estimator_\n",
                "split_idx = int(len(X_selected) * 0.8)\n",
                "X_train, X_test = X_selected.iloc[:split_idx], X_selected.iloc[split_idx:]\n",
                "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
                "\n",
                "best_model.fit(X_train, y_train)\n",
                "y_pred = best_model.predict(X_test)\n",
                "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
                "print(f\"Test RMSE: {rmse:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. モデル解釈 (SHAP Analysis)\n",
                "Random Forestモデルの予測根拠をSHAPで可視化します。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    import shap\n",
                "    # Pipelineからモデルを取り出す\n",
                "    rf_model = best_model.named_steps['regressor']\n",
                "    \n",
                "    # Scalerを通したデータが必要なため、変換を行う\n",
                "    scaler = best_model.named_steps['scaler']\n",
                "    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
                "    \n",
                "    explainer = shap.TreeExplainer(rf_model)\n",
                "    shap_values = explainer.shap_values(X_test_scaled)\n",
                "    \n",
                "    plt.figure(figsize=(10, 6))\n",
                "    shap.summary_plot(shap_values, X_test_scaled, show=False)\n",
                "    plt.title('SHAP Summary Plot (Random Forest)')\n",
                "    plt.show()\n",
                "except ImportError:\n",
                "    print(\"SHAP not installed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. モデル学習と検証 (分類 - Imbalanced Data)\n",
                "RandomForestClassifierを用いた分類タスク（不均衡データ対応）です。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "threshold = y.quantile(0.9)\n",
                "y_class = (y.shift(-1) > threshold).astype(int).iloc[:-1]\n",
                "X_class = X_selected.iloc[:-1]\n",
                "\n",
                "split_idx_c = int(len(X_class) * 0.8)\n",
                "X_train_c, X_test_c = X_class.iloc[:split_idx_c], X_class.iloc[split_idx_c:]\n",
                "y_train_c, y_test_c = y_class.iloc[:split_idx_c], y_class.iloc[split_idx_c:]\n",
                "\n",
                "# Class Weight Balanced\n",
                "clf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=SEED)\n",
                "clf.fit(X_train_c, y_train_c)\n",
                "y_pred_c = clf.predict(X_test_c)\n",
                "\n",
                "print(classification_report(y_test_c, y_pred_c))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. 最終評価と結果の保存\n",
                "結果を保存します。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
                "results_df.to_csv('rf_forecast_results.csv')\n",
                "print(\"Saved to rf_forecast_results.csv\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}