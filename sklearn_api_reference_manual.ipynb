{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Scikit-Learn API 究極リファレンス (Ultimate Sklearn API Reference Manual)\n",
                "\n",
                "このノートブックは、Pythonの標準的な機械学習ライブラリである **Scikit-learn (sklearn)** の主要機能を網羅した「究極のAPIリファレンス」です。\n",
                "データの前処理から、モデルの構築、評価、チューニング、そしてパイプライン化まで、実務で頻繁に使用するコードパターンを辞書形式でまとめています。\n",
                "\n",
                "## 目次\n",
                "1. [データの前処理 (Data Preparation)](#1.-データの前処理-(Data-Preparation))\n",
                "2. [特徴量の選択と削減 (Feature Selection & Reduction)](#2.-特徴量の選択と削減-(Feature-Selection-&-Reduction))\n",
                "3. [モデル選択とデータ分割 (Model Selection & Split)](#3.-モデル選択とデータ分割-(Model-Selection-&-Split))\n",
                "4. [教師あり学習 - 回帰 (Regression)](#4.-教師あり学習---回帰-(Regression))\n",
                "5. [教師あり学習 - 分類 (Classification)](#5.-教師あり学習---分類-(Classification))\n",
                "6. [教師なし学習 - クラスタリング (Clustering)](#6.-教師なし学習---クラスタリング-(Clustering))\n",
                "7. [モデル評価 (Model Evaluation)](#7.-モデル評価-(Model-Evaluation))\n",
                "8. [ハイパーパラメータチューニング (Hyperparameter Tuning)](#8.-ハイパーパラメータチューニング-(Hyperparameter-Tuning))\n",
                "9. [パイプライン (Pipelines & Composites)](#9.-パイプライン-(Pipelines-&-Composites))\n",
                "10. [モデルの保存と読み込み (Model Persistence)](#10.-モデルの保存と読み込み-(Model-Persistence))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. セットアップ\n",
                "必要なライブラリとダミーデータを準備します。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "np.random.seed(42)\n",
                "\n",
                "# ダミーデータの生成 (回帰用と分類用)\n",
                "from sklearn.datasets import make_regression, make_classification\n",
                "\n",
                "# 回帰データ\n",
                "X_reg, y_reg = make_regression(n_samples=200, n_features=10, noise=0.1, random_state=42)\n",
                "\n",
                "# 分類データ\n",
                "X_clf, y_clf = make_classification(n_samples=200, n_features=10, n_classes=2, random_state=42)\n",
                "\n",
                "print(\"Setup Complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. データの前処理 (Data Preparation)\n",
                "モデルに入力する前のデータ変換。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
                "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
                "from sklearn.impute import SimpleImputer, KNNImputer\n",
                "\n",
                "# --- スケーリング (Scaling) ---\n",
                "# 標準化 (平均0, 分散1)\n",
                "scaler_std = StandardScaler()\n",
                "X_std = scaler_std.fit_transform(X_reg)\n",
                "\n",
                "# 正規化 (0-1の範囲)\n",
                "scaler_mm = MinMaxScaler()\n",
                "X_mm = scaler_mm.fit_transform(X_reg)\n",
                "\n",
                "# Robust Scaler (外れ値に強い、四分位範囲を使用)\n",
                "scaler_rob = RobustScaler()\n",
                "X_rob = scaler_rob.fit_transform(X_reg)\n",
                "\n",
                "# --- エンコーディング (Encoding) ---\n",
                "cats = [['Male', 'S'], ['Female', 'M'], ['Female', 'L']]\n",
                "\n",
                "# One-Hot Encoding (ダミー変数化)\n",
                "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
                "cats_ohe = ohe.fit_transform(cats)\n",
                "\n",
                "# Label Encoding (ターゲット変数のラベル化: 'A', 'B' -> 0, 1)\n",
                "le = LabelEncoder()\n",
                "y_le = le.fit_transform(['cat', 'dog', 'cat'])\n",
                "\n",
                "# Ordinal Encoding (順序特徴量: 'S', 'M', 'L' -> 0, 1, 2)\n",
                "oe = OrdinalEncoder(categories=[['S', 'M', 'L']]) # 順序を指定可能\n",
                "cats_oe = oe.fit_transform([['S'], ['L'], ['M']])\n",
                "\n",
                "# --- 欠損値補完 (Imputation) ---\n",
                "X_nan = X_reg.copy()\n",
                "X_nan[0, 0] = np.nan\n",
                "\n",
                "# 単純な補完 (平均、中央値、最頻値)\n",
                "imp_mean = SimpleImputer(strategy='mean')\n",
                "X_imp = imp_mean.fit_transform(X_nan)\n",
                "\n",
                "# KNN補完 (近傍点の平均)\n",
                "imp_knn = KNNImputer(n_neighbors=5)\n",
                "X_knn = imp_knn.fit_transform(X_nan)\n",
                "\n",
                "print(\"Preprocessing examples executed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. 特徴量の選択と削減 (Feature Selection & Reduction)\n",
                "重要な特徴量を選び、次元を削減する。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.feature_selection import SelectKBest, f_regression, RFE, SelectFromModel\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "\n",
                "# --- 特徴量選択 (Selection) ---\n",
                "# 統計的検定 (F値) で上位K個を選択\n",
                "selector_k = SelectKBest(score_func=f_regression, k=5)\n",
                "X_kbest = selector_k.fit_transform(X_reg, y_reg)\n",
                "\n",
                "# RFE (再帰的特徴量削減)\n",
                "estimator = RandomForestRegressor(n_estimators=10, random_state=42)\n",
                "selector_rfe = RFE(estimator, n_features_to_select=5, step=1)\n",
                "X_rfe = selector_rfe.fit_transform(X_reg, y_reg)\n",
                "\n",
                "# SelectFromModel (モデルの重要度に基づく選択)\n",
                "selector_sfm = SelectFromModel(estimator, threshold='median')\n",
                "X_sfm = selector_sfm.fit_transform(X_reg, y_reg)\n",
                "\n",
                "# --- 次元削減 (Reduction) ---\n",
                "# PCA (主成分分析)\n",
                "pca = PCA(n_components=0.95) # 分散の95%を説明できる次元数まで削減\n",
                "X_pca = pca.fit_transform(X_reg)\n",
                "\n",
                "print(f\"Original Shape: {X_reg.shape}\")\n",
                "print(f\"PCA Shape: {X_pca.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. モデル選択とデータ分割 (Model Selection & Split)\n",
                "学習用とテスト用のデータ分割、交差検証の準備。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, TimeSeriesSplit\n",
                "\n",
                "# 単純な分割 (Hold-out)\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X_clf, y_clf, test_size=0.2, random_state=42, stratify=y_clf\n",
                ")\n",
                "\n",
                "# K-Fold (回帰用)\n",
                "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
                "for train_index, val_index in kf.split(X_reg):\n",
                "    pass # ここで学習・検証を行う\n",
                "\n",
                "# Stratified K-Fold (分類用: クラス比率を維持)\n",
                "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
                "\n",
                "# TimeSeriesSplit (時系列用: 未来のデータを検証に使わない)\n",
                "tscv = TimeSeriesSplit(n_splits=5)\n",
                "for train_index, val_index in tscv.split(X_reg):\n",
                "    pass\n",
                "\n",
                "print(\"Split examples executed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. 教師あり学習 - 回帰 (Regression)\n",
                "数値を予測するモデル。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
                "from sklearn.svm import SVR\n",
                "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
                "\n",
                "# 線形回帰\n",
                "lr = LinearRegression()\n",
                "lr.fit(X_train, y_train)\n",
                "\n",
                "# 正則化付き線形回帰 (Ridge: L2, Lasso: L1)\n",
                "ridge = Ridge(alpha=1.0)\n",
                "lasso = Lasso(alpha=0.1)\n",
                "\n",
                "# サポートベクター回帰 (SVR)\n",
                "svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
                "\n",
                "# ランダムフォレスト (並列化可能: n_jobs=-1)\n",
                "rf_reg = RandomForestRegressor(n_estimators=100, max_depth=10, n_jobs=-1, random_state=42)\n",
                "\n",
                "# 勾配ブースティング (GBDT)\n",
                "gb_reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
                "\n",
                "print(\"Regression models initialized.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. 教師あり学習 - 分類 (Classification)\n",
                "クラス（ラベル）を予測するモデル。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "\n",
                "# ロジスティック回帰 (線形分類)\n",
                "log_reg = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
                "log_reg.fit(X_train, y_train)\n",
                "\n",
                "# サポートベクターマシン (SVC)\n",
                "# probability=Trueで確率を出力可能にする（遅くなるので注意）\n",
                "svc = SVC(kernel='rbf', C=1.0, probability=True)\n",
                "\n",
                "# K近傍法 (KNN)\n",
                "knn = KNeighborsClassifier(n_neighbors=5)\n",
                "\n",
                "# 決定木\n",
                "dt = DecisionTreeClassifier(max_depth=5)\n",
                "\n",
                "# ランダムフォレスト分類\n",
                "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "\n",
                "print(\"Classification models initialized.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. 教師なし学習 - クラスタリング (Clustering)\n",
                "データのグループ化。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
                "\n",
                "# K-Means (K平均法)\n",
                "kmeans = KMeans(n_clusters=3, random_state=42)\n",
                "clusters_km = kmeans.fit_predict(X_reg)\n",
                "\n",
                "# DBSCAN (密度ベース)\n",
                "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
                "clusters_db = dbscan.fit_predict(X_reg)\n",
                "\n",
                "# 階層的クラスタリング\n",
                "agg = AgglomerativeClustering(n_clusters=3)\n",
                "clusters_agg = agg.fit_predict(X_reg)\n",
                "\n",
                "print(f\"KMeans Labels: {np.unique(clusters_km)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. モデル評価 (Model Evaluation)\n",
                "予測精度の測定。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
                "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
                "from sklearn.metrics import silhouette_score\n",
                "\n",
                "# --- 回帰評価 ---\n",
                "y_pred_reg = lr.predict(X_test)\n",
                "mse = mean_squared_error(y_test, y_pred_reg)\n",
                "rmse = np.sqrt(mse)\n",
                "mae = mean_absolute_error(y_test, y_pred_reg)\n",
                "r2 = r2_score(y_test, y_pred_reg)\n",
                "print(f\"Regression R2: {r2:.4f}\")\n",
                "\n",
                "# --- 分類評価 ---\n",
                "y_pred_clf = log_reg.predict(X_test)\n",
                "y_prob_clf = log_reg.predict_proba(X_test)[:, 1]\n",
                "\n",
                "acc = accuracy_score(y_test, y_pred_clf)\n",
                "f1 = f1_score(y_test, y_pred_clf)\n",
                "roc_auc = roc_auc_score(y_test, y_prob_clf)\n",
                "cm = confusion_matrix(y_test, y_pred_clf)\n",
                "\n",
                "print(f\"Classification Accuracy: {acc:.4f}\")\n",
                "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_clf))\n",
                "\n",
                "# --- クラスタリング評価 ---\n",
                "sil = silhouette_score(X_reg, clusters_km)\n",
                "print(f\"Silhouette Score: {sil:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. ハイパーパラメータチューニング (Hyperparameter Tuning)\n",
                "最適なパラメータの探索。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
                "\n",
                "# パラメータグリッド\n",
                "param_grid = {\n",
                "    'n_estimators': [50, 100],\n",
                "    'max_depth': [3, 5, 10],\n",
                "    'min_samples_split': [2, 5]\n",
                "}\n",
                "\n",
                "# Grid Search (全探索)\n",
                "grid_search = GridSearchCV(\n",
                "    estimator=RandomForestClassifier(random_state=42),\n",
                "    param_grid=param_grid,\n",
                "    cv=3,\n",
                "    scoring='accuracy',\n",
                "    n_jobs=-1\n",
                ")\n",
                "grid_search.fit(X_train, y_train)\n",
                "\n",
                "print(f\"Best Params: {grid_search.best_params_}\")\n",
                "print(f\"Best Score: {grid_search.best_score_:.4f}\")\n",
                "\n",
                "# Randomized Search (ランダム探索 - 高速)\n",
                "rand_search = RandomizedSearchCV(\n",
                "    estimator=RandomForestClassifier(random_state=42),\n",
                "    param_distributions=param_grid,\n",
                "    n_iter=5, # 試行回数\n",
                "    cv=3,\n",
                "    random_state=42\n",
                ")\n",
                "rand_search.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. パイプライン (Pipelines & Composites)\n",
                "前処理とモデルを連結して一つの推論器として扱う。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.pipeline import Pipeline, make_pipeline\n",
                "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
                "\n",
                "# --- Column Transformer ---\n",
                "# 数値列とカテゴリ列で異なる前処理を適用\n",
                "num_cols = [0, 1, 2] # 列インデックスまたは列名\n",
                "cat_cols = [3]\n",
                "\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', StandardScaler(), num_cols),\n",
                "        ('cat', OneHotEncoder(), cat_cols)\n",
                "    ]\n",
                ")\n",
                "\n",
                "# --- Pipeline ---\n",
                "# 前処理 -> 特徴量選択 -> モデル\n",
                "pipe = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('selector', SelectKBest(k=5)),\n",
                "    ('classifier', LogisticRegression())\n",
                "])\n",
                "\n",
                "# 学習と予測\n",
                "pipe.fit(X_train, y_train)\n",
                "pred = pipe.predict(X_test)\n",
                "\n",
                "print(\"Pipeline executed successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. モデルの保存と読み込み (Model Persistence)\n",
                "学習済みモデルの永続化。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import joblib\n",
                "\n",
                "# 保存\n",
                "joblib.dump(pipe, 'model_pipeline.pkl')\n",
                "\n",
                "# 読み込み\n",
                "loaded_pipe = joblib.load('model_pipeline.pkl')\n",
                "\n",
                "# 再利用\n",
                "result = loaded_pipe.score(X_test, y_test)\n",
                "print(f\"Loaded Model Score: {result:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}